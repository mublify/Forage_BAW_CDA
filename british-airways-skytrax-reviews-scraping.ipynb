{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5228ef86",
   "metadata": {
    "papermill": {
     "duration": 0.003733,
     "end_time": "2024-03-13T06:53:04.784413",
     "exception": false,
     "start_time": "2024-03-13T06:53:04.780680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e32137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T06:53:04.793414Z",
     "iopub.status.busy": "2024-03-13T06:53:04.792895Z",
     "iopub.status.idle": "2024-03-13T06:53:06.197696Z",
     "shell.execute_reply": "2024-03-13T06:53:06.196413Z"
    },
    "papermill": {
     "duration": 1.412959,
     "end_time": "2024-03-13T06:53:06.201045",
     "exception": false,
     "start_time": "2024-03-13T06:53:04.788086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9854085c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T06:53:06.209572Z",
     "iopub.status.busy": "2024-03-13T06:53:06.208962Z",
     "iopub.status.idle": "2024-03-13T06:55:13.916717Z",
     "shell.execute_reply": "2024-03-13T06:55:13.914977Z"
    },
    "papermill": {
     "duration": 127.715194,
     "end_time": "2024-03-13T06:55:13.919544",
     "exception": false,
     "start_time": "2024-03-13T06:53:06.204350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 100 total reviews\n",
      "Scraping page 2\n",
      "   ---> 200 total reviews\n",
      "Scraping page 3\n",
      "   ---> 300 total reviews\n",
      "Scraping page 4\n",
      "   ---> 400 total reviews\n",
      "Scraping page 5\n",
      "   ---> 500 total reviews\n",
      "Scraping page 6\n",
      "   ---> 600 total reviews\n",
      "Scraping page 7\n",
      "   ---> 700 total reviews\n",
      "Scraping page 8\n",
      "   ---> 800 total reviews\n",
      "Scraping page 9\n",
      "   ---> 900 total reviews\n",
      "Scraping page 10\n",
      "   ---> 1000 total reviews\n",
      "Scraping page 11\n",
      "   ---> 1100 total reviews\n",
      "Scraping page 12\n",
      "   ---> 1200 total reviews\n",
      "Scraping page 13\n",
      "   ---> 1300 total reviews\n",
      "Scraping page 14\n",
      "   ---> 1400 total reviews\n",
      "Scraping page 15\n",
      "   ---> 1500 total reviews\n",
      "Scraping page 16\n",
      "   ---> 1600 total reviews\n",
      "Scraping page 17\n",
      "   ---> 1700 total reviews\n",
      "Scraping page 18\n",
      "   ---> 1800 total reviews\n",
      "Scraping page 19\n",
      "   ---> 1900 total reviews\n",
      "Scraping page 20\n",
      "   ---> 2000 total reviews\n",
      "Scraping page 21\n",
      "   ---> 2100 total reviews\n",
      "Scraping page 22\n",
      "   ---> 2200 total reviews\n",
      "Scraping page 23\n",
      "   ---> 2300 total reviews\n",
      "Scraping page 24\n",
      "   ---> 2400 total reviews\n",
      "Scraping page 25\n",
      "   ---> 2500 total reviews\n",
      "Scraping page 26\n",
      "   ---> 2600 total reviews\n",
      "Scraping page 27\n",
      "   ---> 2700 total reviews\n",
      "Scraping page 28\n",
      "   ---> 2800 total reviews\n",
      "Scraping page 29\n",
      "   ---> 2900 total reviews\n",
      "Scraping page 30\n",
      "   ---> 3000 total reviews\n",
      "Scraping page 31\n",
      "   ---> 3100 total reviews\n",
      "Scraping page 32\n",
      "   ---> 3200 total reviews\n",
      "Scraping page 33\n",
      "   ---> 3300 total reviews\n",
      "Scraping page 34\n",
      "   ---> 3400 total reviews\n",
      "Scraping page 35\n",
      "   ---> 3500 total reviews\n",
      "Scraping page 36\n",
      "   ---> 3600 total reviews\n",
      "Scraping page 37\n",
      "   ---> 3700 total reviews\n",
      "Scraping page 38\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 39\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 40\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 41\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 42\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 43\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 44\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 45\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 46\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 47\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 48\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 49\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 50\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 51\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 52\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 53\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 54\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 55\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 56\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 57\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 58\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 59\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 60\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 61\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 62\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 63\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 64\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 65\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 66\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 67\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 68\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 69\n",
      "   ---> 3765 total reviews\n",
      "Scraping page 70\n",
      "   ---> 3765 total reviews\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 70\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "# for i in range(1, pages + 1):\n",
    "for i in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "    \n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e1ab68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T06:55:13.942771Z",
     "iopub.status.busy": "2024-03-13T06:55:13.942287Z",
     "iopub.status.idle": "2024-03-13T06:55:13.973060Z",
     "shell.execute_reply": "2024-03-13T06:55:13.971544Z"
    },
    "papermill": {
     "duration": 0.046862,
     "end_time": "2024-03-13T06:55:13.976519",
     "exception": false,
     "start_time": "2024-03-13T06:55:13.929657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified |  Absolutely horrible custome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Verified |  BA is not what it used to be! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |  BA First, it's not even the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ Trip Verified |  The worst business class ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not Verified |  Quite possibly the worst busin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  ✅ Trip Verified |  Absolutely horrible custome...\n",
       "1  Not Verified |  BA is not what it used to be! ...\n",
       "2  ✅ Trip Verified |  BA First, it's not even the...\n",
       "3  ✅ Trip Verified |  The worst business class ex...\n",
       "4  Not Verified |  Quite possibly the worst busin..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc03c8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T06:55:14.001028Z",
     "iopub.status.busy": "2024-03-13T06:55:14.000509Z",
     "iopub.status.idle": "2024-03-13T06:55:14.187484Z",
     "shell.execute_reply": "2024-03-13T06:55:14.185792Z"
    },
    "papermill": {
     "duration": 0.202625,
     "end_time": "2024-03-13T06:55:14.190724",
     "exception": false,
     "start_time": "2024-03-13T06:55:13.988099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6b875",
   "metadata": {
    "papermill": {
     "duration": 0.011187,
     "end_time": "2024-03-13T06:55:14.212157",
     "exception": false,
     "start_time": "2024-03-13T06:55:14.200970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 3765 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 133.693254,
   "end_time": "2024-03-13T06:55:15.048598",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-13T06:53:01.355344",
   "version": "2.5.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
